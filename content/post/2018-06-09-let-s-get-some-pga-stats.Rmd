---
title: Letâ€™s get some PGA stats
author: ''
date: '2018-06-09'
slug: let-s-get-some-pga-stats
categories:
  - lesson
  - R
tags:
  - rvest
  - purrr
  - map
  - tidy data
  - functionals
thumbnailImagePosition: left
thumbnailImage: http://res.cloudinary.com/dn83gtg0l/image/upload/v1528601498/augusta13th.jpg
summary: "Programmatically finding links, scraping the data, and returning tidy data. All thanks to the `map()` function..."
output:
  blogdown::html_page:
    toc: true
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(tidyverse, rvest)

load("~/Github/blogdown_content/static/data/pga/pga_data.RData")
```

Building off of the [last post](https://taylorgrant.netlify.com/2018/05/using-rvest-to-import-and-html-table/) about using `rvest` to easily scrape html tables, I wanted to extend the process so that I could programmatically scrape multiple tables from multiple pages while still returning tidy data.

For this exercise my source of data is going to be the [PGA Tour](https://www.pgatour.com/){target="_blank"} website and its annual statistics. Not only are there literally hundreds of links to tour stats, but these stats range across multiple years and decades. Each statistic has its own url, and each stat-year has its own path. 

By the end of this, I'll have effectively built a directory of all PGA statistics. Each row of the tibble will contain the stat name as well as a nested tibble with all links to all years of available data. This nested tibble can then be used to query as many, or as few, statistics as necessary. 

In order to do this I need a few functions, and we'll deal with each before putting them all together.     

# Getting the link structure

```{r}
pacman::p_load(tidyverse, rvest, janitor)
```

I think that PGA Tour once had an API, but in poking around I wasn't able to find any current public API, so scraping looks like an appropriate way of getting some data. Before beginning, I'll first use the `robotstxt` package to ensure that I'm actually allowed to scrape the site. The  `paths_allowed()` function will call on the robots.txt file at the root of a domain (i.e., https://www.pgatour.com/robots.txt) to determine if web crawlers, spiders, etc., are allowed to access certain paths on that domain.

```{r, warning = FALSE}
robotstxt::paths_allowed(
  domain = "pgatour.com",
  path = "/",
  bot = "*"
)
```

Ok, so it looks like we're ok to move forward. Let's start by figuring out where the statistics are within the site as well as the structure of the urls of each. 

If we navigate to the [stats](https://www.pgatour.com/stats.html) page for the PGA Tour we can see that there is a main navigation bar below the hero image that contains 9 tabs linked to different stat categories. We'll ignore the Overview tab, which only contains current Top-5 type stats, and focus on the other 8 categories.  

<img src = "/img/pga_stats.png"></img>

Each of these tabs has a similar url structure with only one portion of the path changing based upon the stat category. We can easily create a vector of our stat categories.   

```{r}
base_url <- "https://www.pgatour.com/stats/categories."
categories <- c("ROTT_INQ.html", "RAPP_INQ.html", "RARG_INQ.html",
                "RPUT_INQ.html", "RSCR_INQ.html", "RSTR_INQ.html",
                "RMNY_INQ.html", "RPTS_INQ.html")

link_urls <- tibble(
  cat = c("off_the_tee", "approach_the_green", "around_the_green", 
          "putting", "scoring", "streaks", "money_finishes", "points_rankings"),
  links = str_c(base_url, categories))
```

Within each category page lie the links to our stats of interest, but how do we collect them all? 

Let's start with one category link and use the `rvest::html_attr` function to pull out all of the links on the page and inspect their structure.  

```{r, eval = TRUE}
url_1 <- "https://www.pgatour.com/stats/categories.ROTT_INQ.html"
ott <- url_1 %>% read_html() %>%
    html_nodes(".clearfix a") %>% # we know the css by using Chrome's dev tools
    html_attr('href') %>% 
  as_tibble() %>% 
  mutate(url_loc = row_number())
ott[c(1:4, 35:38, 80:83),1:2]
```
It's clear that `rvest` is working from top to bottom as it collects all of the links. Links 3 and 4 are links to two of the category pages, and links 80 through 83 are tied to social icons found in the footer of the page. Our interest is in the links in the middle that include `/stats/stat/`. These are our statistics. 

<img src = "/img/pga_stat_year.png"></img>

If we visit one of the stats pages and toggle the year we can see the overall url structure:
`[domain]/[path].[stat_id].[year].html`. So as a first step, we need to write a function that visits each category page, grabs the appropriate links, and then mutates them into this basic form. 

This is what `get_stat_links` will accomplish. 

```{r, eval = FALSE}
get_stat_links <- function(l, cat) {
  link <- l %>% read_html() %>%
    html_nodes(".clearfix a") %>% 
    html_attr('href') %>%
    as_tibble() %>%
    filter(str_detect(value, "stats/stat")) %>%
    mutate(value = gsub("/.*/(.*)/", "", value)) %>%
    mutate(cat = cat, 
           link = paste0("https://www.pgatour.com/stats/", value),
           partial = str_replace_all(link, "html", "")) %>% 
    select(-value)
  return(link)
  }
```

We can now use this function to iterate over each category and return all links. Because I'm interested in being able to filter my future directory by category, I'm passing two arguments into my function - the link as well as the category name - and this means that I have to use the `map2()` function. 

```{r, eval = FALSE}
stat_links_list <- map2(link_urls$links, link_urls$cat, get_stat_links)
```

Running this function returns a list of 8 elements, one for each category, and within each list we can find the category name, the link to the overall stat, and the partial url that we're going to eventually use to gather data for all years. 

```{r, echo = FALSE}
stat_links_list
```

We can now bind the rows together to move this from a list to a dataframe. 

```{r, eval = FALSE}
stat_links <- stat_links_list %>% reduce(bind_rows) %>%
  distinct(link, .keep_all = TRUE)
```

# Getting the stat names

With the "partial" column above, we now have the overall format that we'll use to build each statistic-year. But there is an additional wrinkle - the url structure doesn't include the name of the statistic, and the statistic isn't included within the table. We're going to need this information if the directory is going to have any use.  

With the "link" column above we can use `rvest` to grab the name of each stat.   

```{r, eval = FALSE}
get_stat_name <- function(ll) {
  out <- ll %>%
    read_html() %>%
    html_nodes(".current") %>%
    html_text() 
  return(out)
}
```

And we'll use the `map()` function to iterate over all the links and return the statistic names. 

```{r, eval = FALSE}
stat_names <- stat_links$link %>% 
    map(get_stat_name)
```

# Generate links for each stat year

The last function that we need is one that simply takes each partial url from above and appends all possible stat years to it. The earliest year for which stats are available is 1980, and because we don't know when the data for each stat begins, we're just going to assume that each begin in 1980 and ends in 2018. 

Note that this function includes the `nest()` function within it. When we put this all together, we're going to use this function to create nested tibble for each statistic. As we'll see, this will leave us with a much cleaner, more manageable directory to all possible statistics.  

```{r eval = FALSE}
app_yrs <- function(d){
  yrs <- seq(1980, 2018,1)
  out <- str_c(d, yrs, ".html") %>%
    as_tibble() %>%
    nest() %>%
    flatten_df %>% 
    mutate(year = yrs)
  return(out)
}
``` 

# Putting this all together

Now that we have defined each step of the process we can consolidate everything into a single function that will return a `tbl_df`. We are going to use each of the functions described above. First, `get_stat_links` will gather category names as well as the general links to each stat within each category. Then `get_stat_name` is going to pull the names of each statistic and bind them to the general links. Then, for each statistic, we're going to use `app_yrs` to create a url for each possible year and nest all of these links within in each row. Finally, after a little cleanup we've created our directory. 

```{r, eval = FALSE}
stat_data <- function(links) {
  
  stat_links_list <- map2(link_urls$links, link_urls$cat, get_stat_links)
  
  ## get out of list form into dataframe 
  stat_links_list <- stat_links_list[!is.na(stat_links_list)]
  stat_links <- stat_links_list %>% 
    reduce(bind_rows) %>% 
    distinct(link, .keep_all = TRUE) # de-duplicate the links
  
  # get stat name
  stat_names <- stat_links$link %>% 
    map(get_stat_name) 
  
  df <- stat_links %>%
    bind_cols(title = unlist(stat_names)) %>% # unlist stat names and bind cols 
    mutate(year_links = map(partial, app_yrs)) %>% # 
    select(-c(link:partial))
  return(df)
}

dir_links <- stat_data(link_urls)
head(dir_links)
```

```{r, echo = FALSE}
head(dir_links)
```

This is nice and tidy, each row is a different statistic, and nested within the `year_links` column is the link to each year's data table.  We can see what is going on within the nested tibbles by expanding one of them.  

```{r, eval = FALSE}
dir_links %>% filter(title == "Driving Distance") %>%
  unnest(year_links) %>% head()
```

```{r, echo = FALSE}
dir_links %>% filter(title == "Driving Distance") %>%
  unnest(year_links) %>% head()
```

# Using the directory

So now that we have the completed directory, we just need to put it to work. This will require one last function. The `pull_data` function will take each stat-year link from our directory and then capture the html table containing the statistics for that year.  

```{r, eval = FALSE}
pull_data <- function(url){
  out <-  url %>%
    read_html() %>%
    html_nodes(xpath = '//*[@id="statsTable"]') %>%
    html_table() %>%
    flatten_df() %>%
    janitor::clean_names()
  return(out)
}
```

We're also going to use error handling for this function. Not every statistic has data going back to 1980 and we don't want our function to fail when a url is invalid.   

```{r, eval = FALSE}
try_pull_data <- possibly(pull_data, otherwise = NA_character_)
```

With this function, we can now grab any and all data that we want. This could be made into a function, but we'll just use dplyr to pipe our commands together. For instance, let's assume we wanted to pull data for Driving Distance. We're going to take our directory, filter for the statistic of interest, and then run our unnested links through our function. We're then going to re-nest our data so that we are left with tidy data. 

```{r, eval = FALSE}
dd <- dir_links %>% 
  filter(title == "Driving Distance") %>% 
  unnest(year_links) %>% 
  mutate(data = map(value, try_pull_data)) %>%
  group_by(title) %>% 
  nest(-c(value))
```

The result is a single row with the data in a nested tibble. 

```{r, echo = FALSE}
dd
```

When we unnest this tibble, we'll find more nested data. This is actually useful if you want to only keep a select number of years, or if you want to see how much data is available within each year. 

```{r, eval = FALSE}
dd %>% unnest() %>% head()
```

```{r, echo = FALSE}
dd %>% unnest() %>% head
```

Or you can simply unnest twice and retrieve all data. 

```{r, eval = FALSE}
dd %>% unnest() %>% 
  unnest() %>% 
  tail(10) %>%
  knitr::kable()
```

```{r, echo = FALSE}
dd %>% unnest() %>% unnest() %>% tail(10) %>% knitr::kable()
```

Because this function returns tidy data, we can pull multiple statistics and each one will return nested data within each row. Let's pull two statistics for Strokes Gained and see that this what happens. 

```{r, eval = FALSE}
sg2 <- dir_links %>% 
  filter(str_detect(title, "SG")) %>% 
  unnest(year_links) %>% 
  mutate(data = map(value, try_pull_data)) %>%
  group_by(title) %>% 
  nest(-c(value))
```

```{r, echo = FALSE}
sg2
```

Strokes Gained is a relatively new statistic that was only first calculated and collected in 2004. But we are trying to pull data since 1980. What is returned for early years for which no data is actually available? 

```{r, eval = FALSE}
sg2 %>% filter(title == "SG: Tee-to-Green") %>% unnest()
```

```{r, echo = FALSE}
sg2 %>% filter(title == "SG: Tee-to-Green") %>% unnest()
```

Very cool! The function simply returns empty tibbles, and when we unnest for a second time, these are simply ignored. So, who led the PGA Tour in Strokes Gained in 2017, both off the tee and from tee to green?

```{r, eval = FALSE}
sg2 %>% unnest() %>%
  unnest() %>% %>%
  group_by(title) %>%
  filter(year == 2017 & rank_this_week == 1) %>% 
  select(title, year, rank = rank_this_week, player_name) %>%
  knitr::kable()
```

```{r, echo = FALSE}
sg2 %>% unnest() %>%
  unnest() %>% 
  group_by(title) %>%
  filter(year == 2017 & rank_this_week == 1) %>% 
  select(title, year, rank = rank_this_week, player_name) %>%
  knitr::kable()
```

