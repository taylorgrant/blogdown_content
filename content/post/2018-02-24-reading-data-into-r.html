---
title: "Reading data into R"
date: '2019-11-15'
slug: reading-data-into-r
categories:
  - R
tags: 
  - data ingest
  - rvest
  - broom
thumbnailImagePosition: left
thumbnailImage: https://res.cloudinary.com/dn83gtg0l/image/upload/v1548283717/copy.jpg
summary: "Different ways to get data into R"
output:
  blogdown::html_page:
    toc: true
---


<div id="TOC">
<ul>
<li><a href="#built-in-data-sets">Built in data sets</a></li>
<li><a href="#reading-in-data-from-a-url">Reading in data from a URL</a><ul>
<li><a href="#reading-in-a-csv">Reading in a csv</a></li>
<li><a href="#reading-in-an-xls-file">Reading in an xls file</a></li>
</ul></li>
<li><a href="#reading-in-a-zip-file">Reading in a zip file</a></li>
<li><a href="#reading-in-from-google-sheets">Reading in from Google Sheets</a></li>
<li><a href="#scraping-data-from-the-web">Scraping data from the web</a></li>
<li><a href="#reading-in-multiple-files-from-folder">Reading in multiple files from folder</a><ul>
<li><a href="#using-the-readr-package">Using the readr package</a></li>
<li><a href="#using-data.table-package">Using data.table package</a></li>
<li><a href="#reading-in-multiple-files-and-skipping-lines">Reading in multiple files and skipping lines</a></li>
<li><a href="#reading-in-multiple-files-with-purrr">Reading in multiple files with <code>purrr</code></a></li>
</ul></li>
</ul>
</div>

<p>This is intended to document the many ways of getting data into R. Here, I want to showcase how data can be read into a session from various sources, without having to download a csv file first. We’ll do it all in one step.</p>
<div id="built-in-data-sets" class="section level1">
<h1>Built in data sets</h1>
<p>There are so many packages that come with datasets, it can be hard to keep track of all of them. While there isn’t any one site that keeps track of all of them, Vincent Arel-Bundock has built out a pretty detailed composite of a lot of them <a href="https://vincentarelbundock.github.io/Rdatasets/datasets.html" target="_blank">here</a>.</p>
<p>As an example, let’s load the airquality dataset and investigate the effects of ozone on air temperature.</p>
<pre class="r"><code>data(airquality) 

ggplot(airquality, aes(x = Ozone, y = Temp)) + 
  geom_point(col = &quot;dodgerblue&quot;) + 
  facet_grid(~Month) + 
  geom_smooth(method = &#39;lm&#39;) + 
  theme_minimal()</code></pre>
<p><img src="/post/2018-02-24-reading-data-into-r_files/figure-html/airquality-1.png" width="672" /></p>
</div>
<div id="reading-in-data-from-a-url" class="section level1">
<h1>Reading in data from a URL</h1>
<div id="reading-in-a-csv" class="section level2">
<h2>Reading in a csv</h2>
<p>As it has become the convention that data should be shared publicly, there are lots of wonderful places to find data online. For instance, here is one <a href="https://github.com/caesar0301/awesome-public-datasets" target="_blank">great collection</a> of data sources. An easy source for good data is the <a href="https://github.com/fivethirtyeight/data" target="_blank">FiveThirtyEight</a> data page on github, which provides data for all of their articles.</p>
<p>Any csv that has a direct link to it can be downloaded using the <code>readr</code> package. In this case, we’ll read in the raw csv used for their <a href="https://fivethirtyeight.com/features/fandango-movies-ratings/" target="_blank">Fandango piece</a> and recreate a version of their data viz.</p>
<pre class="r"><code>fan &lt;- read_csv(&quot;https://raw.githubusercontent.com/fivethirtyeight/data/master/fandango/fandango_score_comparison.csv&quot;)

starseq &lt;- data.frame(Site = c(rep(&quot;Fandango_Stars&quot;,10), rep(&quot;IMDB_norm_round&quot;,10),
                               rep(&quot;Metacritic_norm_round&quot;,10), rep(&quot;RT_norm_round&quot;, 10)),
                               Stars = rep(seq(0.5,5,.5), 4))

fan %&gt;% gather(&quot;Site&quot;, &quot;Stars&quot;, c(7,14,16,18)) %&gt;%
  select(Site, Stars) %&gt;% group_by(Site, Stars) %&gt;% tally %&gt;% 
  mutate(frac = n / nrow(fan)) %&gt;% right_join(starseq) %&gt;%
  mutate(frac = ifelse(is.na(frac), 0, frac)) %&gt;%
  ggplot(aes(x = Stars, y = frac, group = Site)) + 
  # geom_area(stat = &quot;identity&quot;, position = &quot;stack&quot;) + 
  geom_ribbon(aes(ymin = 0, ymax = frac, group = Site), fill = &quot;gray&quot;,
              alpha = .6) + 
    geom_line(aes(col = Site)) + 
  scale_y_continuous(labels = scales::percent) +
  scale_color_manual(values = c(&quot;red&quot;, &quot;blue&quot;, &quot;purple&quot;, &quot;#3F3F3F&quot;),
                     labels = c(&quot;Fandango&quot;, &quot;IMDB&quot;, 
                                &quot;Metacritic&quot;, &quot;Rotten Tomatoes&quot;)) + 
  theme_minimal() + 
    theme(legend.position = &quot;bottom&quot;) + 
  labs(x = &quot;Stars&quot;, y = &quot;&quot;, 
       title = &quot;Fandango LOVES Movies&quot;)</code></pre>
<p><img src="/post/2018-02-24-reading-data-into-r_files/figure-html/fandango-1.png" width="672" /></p>
</div>
<div id="reading-in-an-xls-file" class="section level2">
<h2>Reading in an xls file</h2>
<p>Use <code>httr</code> to send a GET request and write it to tmp file. Then use the <code>readxl</code> package…</p>
<pre class="r"><code>  library(httr)
  url1 &lt;- &quot;https://www2.census.gov/programs-surveys/demo/tables/geographic-mobility/2018/state-to-state-migration/State_to_State_Migrations_Table_2018.xls&quot;
  GET(url1, write_disk(tf &lt;- tempfile(fileext = &quot;.xls&quot;)))
  df &lt;- readxl::read_excel(tf,skip=7)</code></pre>
</div>
</div>
<div id="reading-in-a-zip-file" class="section level1">
<h1>Reading in a zip file</h1>
<p>When datasets are kept in a compressed form like a zip file, R has the capability of downloading and unzipping those files. The first thing we do is create a temporary directory for the data, then download the archive, then unzip the files and read the data.</p>
<p>As an example, we’ll download, read, and plot some of the data from the <a href="http://www.seanlahman.com/baseball-archive/statistics/" target="_blank">Lahman Baseball Database</a></p>
<pre class="r"><code>library(tidyverse)

# create temporary directory for data
ifelse(!dir.exists(&quot;data/unzipped&quot;), dir.create(&quot;data/unzipped&quot;), &quot;Directory already exists...&quot;)

# specify zip URL 
url &lt;- &quot;http://seanlahman.com/files/database/baseballdatabank-2017.1.zip&quot;

# put zip contents into temp file
tmp &lt;- tempfile()
download.file(url, destfile = tmp)
unzip(tmp, exdir = &quot;data/unzipped/.&quot;)</code></pre>
<p>We can keep the unzipped contents in a separate folder if the data is large and time consuming to download.</p>
<pre class="r"><code>path_unzip &lt;- &quot;data/unzipped/data_archive.zip&quot;
ifelse(!file.exists(path_unzip), 
       download.file(url, path_unzip, mode = &quot;wb&quot;),
       &quot;file already exists&quot;)
unzip(path_unzip, exdir = &quot;data/unzipped/.&quot;)</code></pre>
<p>With the data unzipped, we can read specific files in from the directory. In this case, we’ll map out how many players were born in each US state since 1950.</p>
<pre class="r"><code># read in the csv, filter and tally
players &lt;- read_csv(here(&quot;data/unzipped/baseballdatabank-2017.1/core/Master.csv&quot;)) %&gt;%
  filter(birthYear &gt; 1950 &amp; birthCountry == &quot;USA&quot;) %&gt;%
  distinct(playerID, .keep_all = TRUE) %&gt;%
  group_by(birthState) %&gt;% 
  rename(iso_3166_2 = birthState) %&gt;%
  tally %&gt;%
  mutate(logN = log(n))
    
# get map data 
library(pacman)
p_load(albersusa, sf, sp, rgeos, maptools, ggplot2, 
       ggalt, ggthemes, viridis, scales)

us &lt;- usa_composite()
us_map &lt;- fortify(us, region=&quot;name&quot;)
# merge player dataframe into us data
us@data &lt;- us@data %&gt;% left_join(players)

# set the max for the brightest color
max &lt;- max(us@data$n)
min &lt;- min(us@data$n)


# map it out 
gg &lt;- ggplot()
gg &lt;- gg + geom_map(data=us_map, map=us_map,
                    aes(x=long, y=lat, map_id=id),
                    color=&quot;#2b2b2b&quot;, size=0.1, fill=NA)
gg &lt;- gg + theme_map()

gg + 
  geom_map(data=us@data, map=us_map,
           aes(fill=n, map_id=name),
           color=&quot;white&quot;, size=0.1) +
  coord_map(&quot;polyconic&quot;) +
  labs(title = &quot;Number of major leaguers from each state since 1950&quot;) + 
  scale_fill_viridis(trans = &quot;log&quot;, breaks = c(min(players$n), median(players$n), max(players$n)),
                     option = &quot;A&quot;) +
  theme(legend.position=&quot;right&quot;, 
        legend.title = element_blank(),
        legend.key.width=unit(1, &quot;lines&quot;))</code></pre>
<p><img src="/post/2018-02-24-reading-data-into-r_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
</div>
<div id="reading-in-from-google-sheets" class="section level1">
<h1>Reading in from Google Sheets</h1>
<p>An option that may be of use given the prevalence of the gsuite is to read in data directly from Google Sheets using the <code>googlesheets</code> package.</p>
<p>The first step, if you don’t know the name of the sheet you need, or want to make sure that you have the right sheet, is to pull the directory of all sheets. This is equivalent to what you would see were you to visit <a href="https://docs.google.com/spreadsheets/" class="uri">https://docs.google.com/spreadsheets/</a>.</p>
<pre class="r"><code>library(googlesheets)
# this should ask for authentication of an account
gs_ls()</code></pre>
<p>After you’ve authenticated, a tibble is returned with the data on each sheet, including title, author, version, date last updated, and the sheet key.</p>
<p>Next, register the sheet you want to read in - this can be done either through the Title or the Key. Let’s assume I have a sheet called “R&amp;I API Listing” and that’s what I want to read in, I&quot;ll register it:</p>
<pre class="r"><code>library(googlesheets)

api &lt;- gs_title(&quot;R&amp;I API Listing&quot;)</code></pre>
<p>I can then inspect the sheet using the <code>gs_ws_ls()</code> command, which provides the worksheet names for the entire workbook. Each of which can be referenced individually.</p>
<pre class="r"><code>gs_ws_ls(api)
#&gt; [1] &quot;google&quot; &quot;github&quot; &quot;twitter&quot; &quot;facebook&quot; &quot;netbase&quot;</code></pre>
<p>From this listing, I can then choose to read in the data from one of the sheets.</p>
<pre class="r"><code>twitter &lt;- api %&gt;%
  gs_read(ws = &quot;twitter&quot;)
#
str(twitter)
# 
glimpse(twitter)</code></pre>
</div>
<div id="scraping-data-from-the-web" class="section level1">
<h1>Scraping data from the web</h1>
<p>When all else fails, it is also possible to scrape data using the <code>rvest</code> package. Here, we’ll scrape the <a href="https://www.fangraphs.com/" target="_blank">Fangraphs</a> website for the 2017 offensive players leaderboard.</p>
<p>First, we’ll use the <code>robotstxt</code> pacakge to make sure that we’re allowed to scrape the site.</p>
<pre class="r"><code>robotstxt::paths_allowed(&quot;http://www.fangraphs.com/leaders.aspx?&quot;)</code></pre>
<pre><code>## 
 www.fangraphs.com                      No encoding supplied: defaulting to UTF-8.</code></pre>
<pre><code>## [1] TRUE</code></pre>
<p>We are allowed to, so we are going to scrape 3 pages of data from the site. After scraping the first page to see what the data looked like, I found that rows 1 and 3 weren’t necessary, and row 2 is actually the column header. The <code>getData</code> function below clears that up before binding the rows in the loop.</p>
<pre class="r"><code>library(rvest)

urls &lt;- NULL
root &lt;- &quot;https://www.fangraphs.com/leaders.aspx?pos=all&amp;stats=bat&amp;lg=all&amp;qual=y&amp;type=8&amp;season=2017&amp;month=0&amp;season1=2017&amp;ind=0&amp;page=&quot;

for (i in 1:3){
  urls &lt;- c(urls,(paste0(root,i,&quot;_50&quot;)))
}

getData &lt;- function(url){
  fg &lt;- read_html(url) %&gt;%
    html_nodes(&quot;table&quot;) %&gt;%
    .[[13]] %&gt;%
    html_table(fill = FALSE)
  
  colnames(fg) &lt;- fg[2,]
  fg &lt;- fg[-c(1:3),]
}

df &lt;- NULL
for (url in urls){
  df &lt;- rbind(df,getData(url))
}</code></pre>
<p>The data was read in in character format, so we need to clean that up to make it useful. First, we need to get rid of the % signs in two columns.</p>
<pre class="r"><code>df &lt;- df %&gt;% mutate(`BB%` = stringr::str_replace_all(`BB%`, &quot; %&quot;, &quot;&quot;),
              `K%` = stringr::str_replace_all(`K%`, &quot; %&quot;, &quot;&quot;))
df[c(1,4:22)] &lt;- sapply(df[c(1,4:22)], as.numeric)</code></pre>
<p>Now that we have the data, let’s investigate the factors that best explain the variance in the statistic, WAR. We are going to run simple bivariate regresisons, regressing each of our offensive statistics on WAR and sort our models based on their R<sup>2</sup>.</p>
<pre class="r"><code>library(purrr)
library(broom)

df %&gt;% 
  select(-c(`#`, Name, Team, WAR)) %&gt;%  # exclude outcome, non-numeric cols 
  map(~lm(df$WAR ~ .x, data = df)) %&gt;% 
  map(summary) %&gt;% 
  map_dbl(&quot;r.squared&quot;) %&gt;% 
  tidy %&gt;% 
  dplyr::arrange(desc(x)) %&gt;% 
  rename(r.squared = x) %&gt;%
  slice(1:6)</code></pre>
<pre><code>## Warning: &#39;tidy.numeric&#39; is deprecated.
## See help(&quot;Deprecated&quot;)</code></pre>
<pre><code>## # A tibble: 6 x 2
##   names r.squared
##   &lt;chr&gt;     &lt;dbl&gt;
## 1 Off       0.800
## 2 wRC+      0.692
## 3 wOBA      0.678
## 4 OBP       0.616
## 5 SLG       0.527
## 6 R         0.467</code></pre>
<p>Now let’s graph the top 6 bivariate relationships in terms of variance explained by the model.</p>
<pre class="r"><code>df %&gt;% select(WAR, Off, `wRC+`, wOBA, OBP, SLG, R) %&gt;%
  gather(&quot;stat&quot;, &quot;value&quot;, 2:7) %&gt;%
  ggplot(aes(x = WAR, y = value)) + 
  geom_smooth(method = &quot;lm&quot;, se=FALSE, formula = y ~ x) +
  geom_point(col = &quot;dodgerblue&quot;, size = .8) + 
  facet_wrap(~stat, scales = &quot;free&quot;) + 
  theme_minimal() + 
  labs(x = &quot;WAR&quot;, y = &quot;Statistic Value&quot;) + 
  theme(
    strip.background = element_rect(fill=&quot;gray&quot;, colour = &quot;transparent&quot;))</code></pre>
<p><img src="/post/2018-02-24-reading-data-into-r_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>Compare this with the 6 statistics that explain the lowest amount of the variance.</p>
<p><img src="/post/2018-02-24-reading-data-into-r_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
</div>
<div id="reading-in-multiple-files-from-folder" class="section level1">
<h1>Reading in multiple files from folder</h1>
<p>If all csv’s are in the same folder, then it is simple to read them all in and bind them together.</p>
<div id="using-the-readr-package" class="section level3">
<h3>Using the readr package</h3>
<pre class="r"><code>library(tidyverse)
files &lt;- list.files(path = &quot;[if not working directory]&quot;, pattern = &quot;*.csv&quot;)
df &lt;- lapply(files, read_csv) %&gt;% bind_rows()</code></pre>
<p>If the files are large, the readr package will be slower than the <code>data.table</code> package</p>
</div>
<div id="using-data.table-package" class="section level3">
<h3>Using data.table package</h3>
<pre class="r"><code>library(data.table)
files &lt;- list.files(pattern = &quot;*.csv&quot;)
DT = rbindlist(lapply(files, fread))</code></pre>
</div>
<div id="reading-in-multiple-files-and-skipping-lines" class="section level2">
<h2>Reading in multiple files and skipping lines</h2>
<p>If you’re going to be reading in multiple files and binding them, then you need the variable names in the head. With a single file and the <code>readr</code> pacakge, that would be simple:</p>
<pre class="r"><code>df &lt;- readr::read_csv(&quot;filename.csv&quot;, skip = 10)</code></pre>
<p>But with multiple files you write the following after identifying your files.</p>
<pre class="r"><code>df &lt;- do.call(rbind, lapply(files, read_csv, skip = 10))</code></pre>
</div>
<div id="reading-in-multiple-files-with-purrr" class="section level2">
<h2>Reading in multiple files with <code>purrr</code></h2>
<p>The above will work, but working within the tidyverse we can also pipe together most of the information necessary. This is using .RDS files, but the read command can be changed to whatever.</p>
<pre class="r"><code>data_path &lt;- &quot;folder_location&quot;   # path to the data
files &lt;- dir(data_path, pattern = &quot;*.RDS&quot;) # get file names

df &lt;- files %&gt;%
  # read in all the files, appending the path before the filename
  map(~ readRDS(file.path(data_path, .))) %&gt;% 
  reduce(bind_rows)
df</code></pre>
<p><sub>fin</sub></p>
</div>
</div>
