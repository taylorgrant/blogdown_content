---
title: "Reading data into R"
date: '2018-02-24'
slug: reading-data-into-r
categories:
  - R
  - lesson
tags: 
  - data ingest
  - rvest
  - broom
summary: "5 different ways to get data into R"
output:
  blogdown::html_page:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readxl)
library(here)
```

This is intended to document the many ways of getting data into R. Here, I want to showcase how data can be read into a session from various sources, without having to download a csv file first. We'll do it all in one step. 

# Built in data sets 

There are so many packages that come with datasets, it can be hard to keep track of all of them. While there isn't any one site that keeps track of all of them, Vincent Arel-Bundock has built out a pretty detailed composite of a lot of them [here](https://vincentarelbundock.github.io/Rdatasets/datasets.html){target="_blank"}. 

As an example, let's load the airquality dataset and investigate the effects of ozone on air temperature. 

```{r, airquality, warning = FALSE}
data(airquality) 

ggplot(airquality, aes(x = Ozone, y = Temp)) + 
  geom_point(col = "dodgerblue") + 
  facet_grid(~Month) + 
  geom_smooth(method = 'lm') + 
  theme_minimal()
```

# Reading in data from a URL

As it has become the convention that data should be shared publicly, there are lots of wonderful places to find data online. For instance, here is one [great collection](https://github.com/caesar0301/awesome-public-datasets){target="_blank"} of data sources. An easy source for good data is the [FiveThirtyEight](https://github.com/fivethirtyeight/data){target="_blank"} data page on github, which provides data for all of their articles. 

Any csv that has a direct link to it can be downloaded using the ```readr``` package. In this case, we'll read in the raw csv used for their [Fandango piece](https://fivethirtyeight.com/features/fandango-movies-ratings/){target="_blank"} and recreate a version of their data viz. 

```{r , fandango, message = FALSE, warning = FALSE}
fan <- read_csv("https://raw.githubusercontent.com/fivethirtyeight/data/master/fandango/fandango_score_comparison.csv")

starseq <- data.frame(Site = c(rep("Fandango_Stars",10), rep("IMDB_norm_round",10),
                               rep("Metacritic_norm_round",10), rep("RT_norm_round", 10)),
                               Stars = rep(seq(0.5,5,.5), 4))

fan %>% gather("Site", "Stars", c(7,14,16,18)) %>%
  select(Site, Stars) %>% group_by(Site, Stars) %>% tally %>% 
  mutate(frac = n / nrow(fan)) %>% right_join(starseq) %>%
  mutate(frac = ifelse(is.na(frac), 0, frac)) %>%
  ggplot(aes(x = Stars, y = frac, group = Site)) + 
  # geom_area(stat = "identity", position = "stack") + 
  geom_ribbon(aes(ymin = 0, ymax = frac, group = Site), fill = "gray",
              alpha = .6) + 
    geom_line(aes(col = Site)) + 
  scale_y_continuous(labels = scales::percent) +
  scale_color_manual(values = c("red", "blue", "purple", "#3F3F3F"),
                     labels = c("Fandango", "IMDB", 
                                "Metacritic", "Rotten Tomatoes")) + 
  theme_minimal() + 
    theme(legend.position = "bottom") + 
  labs(x = "Stars", y = "", 
       title = "Fandango LOVES Movies")
```

# Reading in a zip file

When datasets are kept in a compressed form like a zip file, R has the capability of downloading and unzipping those files. The first thing we do is create a temporary directory for the data, then download the archive, then unzip the files and read the data. 

As an example, we'll download, read, and plot some of the data from the [Lahman Baseball Database](http://www.seanlahman.com/baseball-archive/statistics/){target="_blank"}

```{r eval = FALSE} 
library(tidyverse)

# create temporary directory for data
ifelse(!dir.exists("data/unzipped"), dir.create("data/unzipped"), "Directory already exists...")

# specify zip URL 
url <- "http://seanlahman.com/files/database/baseballdatabank-2017.1.zip"

# put zip contents into temp file
tmp <- tempfile()
download.file(url, destfile = tmp)
unzip(tmp, exdir = "data/unzipped/.")
```

We can keep the unzipped contents in a separate folder if the data is large and time consuming to download. 

```{r eval = FALSE} 
path_unzip <- "data/unzipped/data_archive.zip"
ifelse(!file.exists(path_unzip), 
       download.file(url, path_unzip, mode = "wb"),
       "file already exists")
unzip(path_unzip, exdir = "data/unzipped/.")
```

With the data unzipped, we can read specific files in from the directory. In this case, we'll map out how many players were born in each US state since 1950. 

```{r, cache = FALSE, map, warning = FALSE, message = FALSE}
# read in the csv, filter and tally
players <- read_csv(here("data/unzipped/baseballdatabank-2017.1/core/Master.csv")) %>%
  filter(birthYear > 1950 & birthCountry == "USA") %>%
  distinct(playerID, .keep_all = TRUE) %>%
  group_by(birthState) %>% 
  rename(iso_3166_2 = birthState) %>%
  tally %>%
  mutate(logN = log(n))
    
# get map data 
library(pacman)
p_load(albersusa, sf, sp, rgeos, maptools, ggplot2, 
       ggalt, ggthemes, viridis, scales)

us <- usa_composite()
us_map <- fortify(us, region="name")
# merge player dataframe into us data
us@data <- us@data %>% left_join(players)

# set the max for the brightest color
max <- max(us@data$n)
min <- min(us@data$n)


# map it out 
gg <- ggplot()
gg <- gg + geom_map(data=us_map, map=us_map,
                    aes(x=long, y=lat, map_id=id),
                    color="#2b2b2b", size=0.1, fill=NA)
gg <- gg + theme_map()

gg + 
  geom_map(data=us@data, map=us_map,
           aes(fill=n, map_id=name),
           color="white", size=0.1) +
  coord_map("polyconic") +
  labs(title = "Number of major leaguers from each state since 1950") + 
  scale_fill_viridis(trans = "log", breaks = c(min(players$n), median(players$n), max(players$n)),
                     option = "A") +
  theme(legend.position="right", 
        legend.title = element_blank(),
        legend.key.width=unit(1, "lines"))
```

# Reading in from Google Sheets

An option that may be of use given the prevalence of the gsuite is to read in data directly from Google Sheets using the ```googlesheets``` package. 

The first step, if you don't know the name of the sheet you need, or want to make sure that you have the right sheet, is to pull the directory of all sheets. This is equivalent to what you would see were you to visit <https://docs.google.com/spreadsheets/>. 

```{r eval = FALSE}
library(googlesheets)
# this should ask for authentication of an account
gs_ls()
```

After you've authenticated, a tibble is returned with the data on each sheet, including title, author, version, date last updated, and the sheet key. 

Next, register the sheet you want to read in - this can be done either through the Title or the Key. Let's assume I have a sheet called "R&I API Listing" and that's what I want to read in, I"ll register it:

```{r eval = FALSE}
library(googlesheets)

api <- gs_title("R&I API Listing")
```
I can then inspect the sheet using the `gs_ws_ls()` command, which provides the worksheet names for the entire workbook. Each of which can be referenced individually. 
```{r eval = FALSE}
gs_ws_ls(api)
#> [1] "google" "github" "twitter" "facebook" "netbase"
```

From this listing, I can then choose to read in the data from one of the sheets. 

```{r eval = FALSE}
twitter <- api %>%
  gs_read(ws = "twitter")
#
str(twitter)
# 
glimpse(twitter)
```

# Scraping data from the web

When all else fails, it is also possible to scrape data using the ```rvest``` package. Here, we'll scrape the [Fangraphs](https://www.fangraphs.com/){target="_blank"} website for the 2017 offensive players leaderboard. 

First, we'll use the ```robotstxt``` pacakge to make sure that we're allowed to scrape the site. 

```{r warning=FALSE}
robotstxt::paths_allowed("http://www.fangraphs.com/leaders.aspx?")
```

We are allowed to, so we are going to scrape 3 pages of data from the site. After scraping the first page to see what the data looked like, I found that rows 1 and 3 weren't necessary, and row 2 is actually the column header. The ```getData``` function below clears that up before binding the rows in the loop. 

```{r, message = FALSE}
library(rvest)

urls <- NULL
root <- "https://www.fangraphs.com/leaders.aspx?pos=all&stats=bat&lg=all&qual=y&type=8&season=2017&month=0&season1=2017&ind=0&page="

for (i in 1:3){
  urls <- c(urls,(paste0(root,i,"_50")))
}

getData <- function(url){
  fg <- read_html(url) %>%
    html_nodes("table") %>%
    .[[12]] %>%
    html_table(fill = FALSE)
  
  colnames(fg) <- fg[2,]
  fg <- fg[-c(1:3),]
}

df <- NULL
for (url in urls){
  df <- rbind(df,getData(url))
}
```

The data was read in in character format, so we need to clean that up to make it useful. First, we need to get rid of the % signs in two columns.  

```{r}
df <- df %>% mutate(`BB%` = stringr::str_replace_all(`BB%`, " %", ""),
              `K%` = stringr::str_replace_all(`K%`, " %", ""))
df[c(1,4:22)] <- sapply(df[c(1,4:22)], as.numeric)
```

Now that we have the data, let's investigate the factors that best explain the variance in the statistic, WAR. We are going to run simple bivariate regresisons, regressing each of our offensive statistics on WAR and sort our models based on their R^2^.

```{r}
library(purrr)
library(broom)

df %>% 
  select(-c(`#`, Name, Team, WAR)) %>%  # exclude outcome, non-numeric cols 
  map(~lm(df$WAR ~ .x, data = df)) %>% 
  map(summary) %>% 
  map_dbl("r.squared") %>% 
  tidy %>% 
  dplyr::arrange(desc(x)) %>% 
  rename(r.squared = x) %>%
  slice(1:6)
```

Now let's graph the top 6 bivariate relationships in terms of variance explained by the model.  

```{r}
df %>% select(WAR, Off, `wRC+`, wOBA, OBP, SLG, R) %>%
  gather("stat", "value", 2:7) %>%
  ggplot(aes(x = WAR, y = value)) + 
  geom_smooth(method = "lm", se=FALSE, formula = y ~ x) +
  geom_point(col = "dodgerblue", size = .8) + 
  facet_wrap(~stat, scales = "free") + 
  theme_minimal() + 
  labs(x = "WAR", y = "Statistic Value") + 
  theme(
    strip.background = element_rect(fill="gray", colour = "transparent"))
```

Compare this with the 6 statistics that explain the lowest amount of the variance.    

```{r, echo = FALSE}
df %>% select(WAR, G, `K%`, SB, BsR, PA, Def) %>%
  gather("stat", "value", 2:7) %>%
  ggplot(aes(x = WAR, y = value)) + 
  geom_smooth(method = "lm", se=FALSE, formula = y ~ x) +
  geom_point(col = "red", size = .8) + 
  facet_wrap(~stat, scales = "free") + 
  theme_minimal() + 
  labs(x = "WAR", y = "Statistic Value") + 
  theme(
    strip.background = element_rect(fill="gray", colour = "transparent"))
```

~fin~